{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cosmos DB Synapse Sales Processing Spark Notebook\r\n",
        "\r\n",
        "## This Spark/PySpark Notebook demonstrates how to:\r\n",
        "\r\n",
        "- **Read the Synapse Link Analytic Datastore with Spark/PySpark in Azure Synapse**\r\n",
        "- **Source Cosmos DB is the Mongo API**\r\n",
        "- **Aggregating the sales data by customer_id**\r\n",
        "- Displaying the \"shape\" of the dataframes, and observed schema\r\n",
        "- Filter the sales data (by doctype, timestamp)\r\n",
        "- Writing the aggregated \"materialized view\" of sales-by-customer to the Cosmos DB views container\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define common PySpark functions used in this Notebook\r\n",
        "\r\n",
        "def print_df_shape(df, msg):\r\n",
        "    print(\"shape of df {} - row count: {}, column count: {}\".format(\r\n",
        "        msg, str(df.count()), str(len(df.columns)) ))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Synapse Link Analytic Datastore into a Spark Dataframe\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# initialize variables; these can be computed values\n",
        "# such as for a daily or monthly report.\n",
        "min_doc_timestamp = 1640995200  # 2022-01-01T00:00:00.000Z\n",
        "max_doc_timestamp = 1999999999  # distant future\n",
        "\n",
        "# Note: \"cosmos.olap\" is Synapse Link, \"cosmos.oltp\" is Cosmos DB\n",
        "\n",
        "sales_df = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"gbbcjmongo_retail\")\\\n",
        "    .option(\"spark.cosmos.container\", \"sales\")\\\n",
        "    .load()\\\n",
        "    .filter(col(\"_ts\") >= min_doc_timestamp)\\\n",
        "    .filter(col(\"_ts\") <= max_doc_timestamp)\n",
        "\n",
        "display(sales_df.limit(10))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape and the schema of the input dataframe\r\n",
        "\r\n",
        "print_df_shape(sales_df, \"sales_df\")\r\n",
        "sales_df.printSchema()\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "import pyspark.sql.functions as F \r\n",
        "\r\n",
        "# The input documents look like this; we'll select just a few\r\n",
        "# of these attributes for processing.\r\n",
        "\r\n",
        "# {\r\n",
        "# \t\"_id\" : ObjectId(\"64458d9959d30378b040b2a5\"),\r\n",
        "# \t\"pk\" : \"1\",\r\n",
        "# \t\"id\" : \"051424eb-9de0-4ff4-810a-522cdd9ece07\",\r\n",
        "# \t\"sale_id\" : 1,\r\n",
        "# \t\"doctype\" : \"line_item\",\r\n",
        "# \t\"date\" : \"2021-01-01\",\r\n",
        "# \t\"line_num\" : 1,\r\n",
        "# \t\"customer_id\" : 6168,\r\n",
        "# \t\"store_id\" : 60,\r\n",
        "# \t\"upc\" : \"0760981980837\",\r\n",
        "# \t\"price\" : 84.65,\r\n",
        "# \t\"qty\" : 1,\r\n",
        "# \t\"cost\" : 84.65,\r\n",
        "# \t\"epoch\" : 1682279833.3812423\r\n",
        "# }\r\n",
        "\r\n",
        "sales_df_unpacked = sales_df.select(\r\n",
        "    col('sale_id.*'),\r\n",
        "    col('customer_id.*'),\r\n",
        "    col('doctype.*'),\r\n",
        "    col('item_count.*'),\r\n",
        "    col('total_cost.*'))\r\n",
        "\r\n",
        "# Rename the columns of the unpacked DataFrame to friendly names\r\n",
        "new_column_names = ['sale_id', 'customer_id', 'doctype', 'item_count', 'total_cost']\r\n",
        "df_sales = sales_df_unpacked.toDF(*new_column_names).filter(col(\"doctype\") == \"sale\")\r\n",
        "\r\n",
        "print_df_shape(df_sales, \"df_sales\")\r\n",
        "display(df_sales.limit(10))\r\n",
        "df_sales.printSchema()\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate Sales by Customer \r\n",
        "\r\n",
        "# But first, consider an alternative implementation using JUST CosmosDB, and not Spark:\r\n",
        "# 1. Read customers container to get the unique set of customer ids (cross partition)\r\n",
        "# 2. Loop through the customer id list:\r\n",
        "#    - Read all of the sale documents for each customer in the timeframe (cross partition)\r\n",
        "#    - sum the sales item_count and item_count for each customer (memory intensive)\r\n",
        "\r\n",
        "import pyspark.sql.functions as F \r\n",
        "\r\n",
        "df_customer_aggregated = df_sales.groupBy(\"customer_id\") \\\r\n",
        "    .agg(\r\n",
        "        F.first('customer_id').alias('id'), \\\r\n",
        "        F.first('customer_id').alias('pk'), \\\r\n",
        "        F.count(\"customer_id\").alias('order_count'), \\\r\n",
        "        F.sum(\"total_cost\").alias(\"total_dollar_amount\"), \\\r\n",
        "        F.sum(\"item_count\").alias(\"total_item_count\")) \\\r\n",
        "        .sort(\"customer_id\", ascending=True)\r\n",
        "\r\n",
        "print_df_shape(df_customer_aggregated, \"df_customer_aggregated\")\r\n",
        "display(df_customer_aggregated.limit(10))\r\n",
        "df_customer_aggregated.printSchema()\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Installed Packages\r\n",
        "\r\n",
        "import pkg_resources\r\n",
        "pkg_list = list()\r\n",
        "\r\n",
        "for d in sorted(pkg_resources.working_set):\r\n",
        "    pkg_list.append(str(d))\r\n",
        "for p in sorted(pkg_list):\r\n",
        "    pass\r\n",
        "    #print(p)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_customer_aggregated.createOrReplaceTempView('agg')\r\n",
        "print('df_customer_aggregated saved to tmp view: agg')\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the use of the %%sql \"magick\" command to use Spark SQL \r\n",
        "\r\n",
        "%%sql\r\n",
        "select * from agg limit 3\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use spark.sql and SELECT to convert id and pk to String values\r\n",
        "\r\n",
        "%%pyspark\r\n",
        "df_for_cosmos = spark.sql(\"SELECT String(id), String(pk), order_count, total_dollar_amount, total_item_count FROM agg\")\r\n",
        "\r\n",
        "print_df_shape(df_for_cosmos, \"df_for_cosmos\")\r\n",
        "display(df_for_cosmos.limit(10))\r\n",
        "df_for_cosmos.printSchema()\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "df_for_cosmos.write.format(\"cosmos.oltp\")\\\r\n",
        "    .option(\"spark.synapse.linkedService\", \"gbbcjcdbnosql_retail_db\")\\\r\n",
        "    .option(\"spark.cosmos.container\", \"views\")\\\r\n",
        "    .mode('append')\\\r\n",
        "    .save()\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}