{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## CosmosDB/Mongo API Sales Data Processing with Synapse Link\n",
        "\n",
        "The purpose of this Spark Notebook is to read the **Synapse Link Sales data** \n",
        "and produce **aggregated totals by Customer** and write these aggregated totals \n",
        "to both **Azure Storage** and **Azure PostgreSQL**.\n",
        "\n",
        "### Programming Logic in this Notebook, by Cell\n",
        "\n",
        "- Define common PySpark functions\n",
        "- Load the Synapse Link Sales data into a Dataframe\n",
        "- Sort by _ts, display the newest Documents\n",
        "- Select just the pertinent columns for the aggregation calculation\n",
        "- Aggregate Sales by Customer\n",
        "- Pass the aggregated DataFrame to Spark/Scala as a TempView\n",
        "- Write the aggregated DataFrame to Azure Storage as CSV \n",
        "- Write the aggregated DataFrame to an Azure PostgreSQL table\n",
        "\n",
        "### Links \n",
        "\n",
        "- https://github.com/Azure-Samples/Synapse/tree/main/Notebooks/PySpark"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define common PySpark functions used in this Notebook\n",
        "\n",
        "def print_df_shape(df, msg):\n",
        "    print(\"shape of df {} - row count: {}, column count: {}\".format(\n",
        "        msg, str(df.count()), str(len(df.columns)) ))\n",
        "\n",
        "def write_df_to_csv_blob(df, out_csv):\n",
        "\n",
        "    # Azure storage account info\n",
        "    blob_account_name   = 'cjoakimstorage'\n",
        "    blob_container_name = 'retail'\n",
        "    blob_relative_path  = 'demo'\n",
        "    linked_service_name = 'SecondaryAzureBlobStorage'\n",
        "\n",
        "    blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(\n",
        "        linked_service_name)\n",
        "    #print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "        blob_container_name, blob_account_name, blob_relative_path)\n",
        "\n",
        "    spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "        blob_container_name, blob_account_name), blob_sas_token)\n",
        "\n",
        "    csv_path = '{}/{}'.format(wasbs_path, out_csv)\n",
        "\n",
        "    print('wasbs_path: ' + wasbs_path)\n",
        "    print('csv_path:   ' + csv_path)\n",
        "\n",
        "    # Write to blob storage, coalesce it into one CSV file\n",
        "    df.coalesce(1).write.csv(csv_path, mode='overwrite', header='true')\n",
        "    print('written')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SynapseLink Sales data into a Dataframe.\n",
        "# Select just the \"sale\" document types from the sales container, \n",
        "# which have a minimum _ts (timestamp) value.\n",
        "\n",
        "# The documents in CosmosDB OLTP data look like this:\n",
        "# { \n",
        "#     \"_id\" : ObjectId(\"6200059edbf78e1f05346e70\"), \n",
        "#     \"pk\" : \"1\", \n",
        "#     \"id\" : \"d6167c84-024a-4ecd-9c95-55f0005615d0\", \n",
        "#     \"sale_id\" : NumberInt(1), \n",
        "#     \"doctype\" : \"sale\", \n",
        "#     \"date\" : \"2021-01-01\", \n",
        "#     \"dow\" : \"fri\", \n",
        "#     \"customer_id\" : NumberInt(3275), \n",
        "#     \"store_id\" : NumberInt(61), \n",
        "#     \"item_count\" : NumberInt(3), \n",
        "#     \"total_cost\" : 2049.71\n",
        "# }\n",
        "\n",
        "# The above document is from this query in Studio 3T:\n",
        "#   db.getCollection(\"sales\").find({doctype:\"sale\"})\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# initialize variables\n",
        "min_doc_timestamp = 1640995200  # 2022-01-01T00:00:00.000Z\n",
        "max_doc_timestamp = 1672531199  # 2022-12-31T23:59:59.999Z\n",
        "\n",
        "# read the raw OLAP data, filtering by document _ts (timestamp)\n",
        "df_sales_raw = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"CosmosMongoDemoDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"sales\")\\\n",
        "    .load()\\\n",
        "    .filter(col(\"_ts\") >= min_doc_timestamp)\\\n",
        "    .filter(col(\"_ts\") <= max_doc_timestamp)\n",
        "\n",
        "print_df_shape(df_sales_raw, \"df_sales_raw\")\n",
        "display(df_sales_raw.limit(20))\n",
        "df_sales_raw.printSchema()\n",
        "\n",
        "# shape of df df_sales_raw - row count: 110025, column count: 20"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by _ts, display the newest Documents\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "sorted_df = df_sales_raw.orderBy(col(\"_ts\").desc())\n",
        "\n",
        "print_df_shape(sorted_df, \"sorted_df\")\n",
        "\n",
        "sorted_df.select(\"sale_id\", \"doctype\", \"date\", \"_ts\").show(30)\n",
        "\n",
        "# |sale_id|    doctype|        date|       _ts|\n",
        "# |{31396}|{line_item}|{2022-02-16}|1644173836|\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select just the pertinent columns for the aggregation calculation.\n",
        "# Unpack the struct columns with attrname.* syntax.\n",
        "\n",
        "df_sales_unpacked = df_sales_raw.select(\n",
        "    col('doctype.*'),\n",
        "    col('date.*'),\n",
        "    col('customer_id.*'),\n",
        "    col('item_count.*'),\n",
        "    col('total_cost.*'))\n",
        "\n",
        "print_df_shape(df_sales_unpacked, \"df_sales_unpacked\")\n",
        "display(df_sales_unpacked.limit(3))\n",
        "df_sales_unpacked.printSchema()\n",
        "\n",
        "# Rename the columns of the unpacked DataFrame\n",
        "new_column_names = [\n",
        "    'doctype', 'date', 'customer_id', 'item_count', 'total_cost']\n",
        "df_sales = df_sales_unpacked.toDF(*new_column_names).filter(col(\"doctype\") == \"sale\")\n",
        "\n",
        "print_df_shape(df_sales, \"df_sales\")\n",
        "display(df_sales.limit(3))\n",
        "df_sales.printSchema()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate Sales by Customer \n",
        "\n",
        "# Consider an alternative implementation using just CosmosDB, and not Spark:\n",
        "# 1. Read customers container to get the unique set of customer ids (cross partition)\n",
        "# 2. Loop through the customer id list:\n",
        "#    - Read all of the sale documents for each customer in the timeframe (cross partition)\n",
        "#    - sum the sales item_count and item_count for each customer (memory intensive?)\n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "df_customer_aggregated = df_sales.groupBy(\"customer_id\") \\\n",
        "    .agg(\n",
        "        F.first('customer_id').alias('_id'), \\\n",
        "        F.first('customer_id').alias('pk'), \\\n",
        "        F.count(\"customer_id\").alias('order_count'), \\\n",
        "        F.sum(\"total_cost\").alias(\"total_dollar_amount\"), \\\n",
        "        F.sum(\"item_count\").alias(\"total_item_count\")) \\\n",
        "        .sort(\"customer_id\", ascending=True)\n",
        "\n",
        "print_df_shape(df_customer_aggregated, \"df_customer_aggregated\")\n",
        "display(df_customer_aggregated.limit(4))\n",
        "df_customer_aggregated.printSchema()\n",
        "\n",
        "# Pass the df_customer_aggregated DataFrame to Spark/Scala as a TempView\n",
        "df_customer_aggregated.createOrReplaceTempView(\"CustomerAggSales\")\n",
        "\n",
        "print('df_customer_aggregated, shape: {} x {}'.format(\n",
        "        df_customer_aggregated.count(), \n",
        "        len(df_customer_aggregated.columns)))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the df_customer_aggregated to Azure Blob Storage as CSV \n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# Azure storage account info\n",
        "blob_account_name   = 'cjoakimstorage'\n",
        "blob_container_name = 'synapse'\n",
        "blob_relative_path  = 'retail/sales/mongo/'\n",
        "linked_service_name = 'SecondaryAzureBlobStorage'\n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "# Allow Spark to access from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "    blob_container_name, blob_account_name, blob_relative_path)\n",
        "\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "    blob_container_name, blob_account_name), blob_sas_token)\n",
        "\n",
        "csv_path = '{}{}'.format(wasbs_path,'sales_by_customer_csv')\n",
        "\n",
        "print('wasbs_path: ' + wasbs_path)\n",
        "print('csv_path:   ' + csv_path)\n",
        "\n",
        "# Write to blob storage, coalesce it into one CSV file\n",
        "df_customer_aggregated.coalesce(1).write.csv(csv_path, mode='overwrite', header='true')\n",
        "print('csv data written to azure storage blob')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark \n",
        "\n",
        "// Obtain configuration values from spark.conf\n",
        "val connStr  = spark.conf.get(\"spark.azurepg.jdbc.connstring\")\n",
        "val driver   = spark.conf.get(\"spark.azurepg.jdbc.driver\")\n",
        "val server   = spark.conf.get(\"spark.azurepg.jdbc.server\")\n",
        "val database = spark.conf.get(\"spark.azurepg.jdbc.database\")\n",
        "val table    = \"public.customer_sales\"\n",
        "val user     = spark.conf.get(\"spark.azurepg.jdbc.user\")\n",
        "val password = spark.conf.get(\"spark.azurepg.jdbc.pass\")\n",
        "\n",
        "// Read the temp table into a Dataframe\n",
        "val df_temp_view = spark.read.table(\"CustomerAggSales\")\n",
        "\n",
        "println(\"df_temp_view row count: \" + df_temp_view.count())\n",
        "\n",
        "// Using JDBC, write the Dataframe to Azure PostgreSQL\n",
        "df_temp_view.write\n",
        "  .format(\"jdbc\")\n",
        "  .option(\"url\", connStr)\n",
        "  .option(\"driver\", driver)\n",
        "  .option(\"dbtable\", table)\n",
        "  .option(\"user\", user)\n",
        "  .option(\"password\", password)\n",
        "  .mode(\"overwrite\")\n",
        "  .save()\n",
        "\n",
        "// In Azure Data Studio, execute this SQL query vs PostgreSQL:\n",
        "// select * from public.customer_sales order by customer_id \n",
        "\n",
        "println(\"done\\n\\n\\n\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}