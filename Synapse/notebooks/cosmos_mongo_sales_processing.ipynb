{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## CosmosDB/Mongo API Sales Data Processing with Synapse Link\n",
        "\n",
        "Purpose is to read the **Synapse Link Sales data** and produce\n",
        "**aggregated totals by Customer**.  Write these aggregated totals to both\n",
        "**Azure Storage and Azure PostgreSQL**.\n",
        "\n",
        "### Programming Logic in this Notebook, by Cell\n",
        "\n",
        "- Define common PySpark functions\n",
        "- Load the Synapse Link Sales data into a Dataframe\n",
        "- Select just the pertinent columns for the aggregation calculation\n",
        "- Aggregate Sales by Customer\n",
        "- Pass the aggregated DataFrame to Spark/Scala as a TempView\n",
        "- Write the aggregated DataFrame to Azure Storage as CSV \n",
        "- Write the aggregated DataFrame to an Azure PostgreSQL table\n",
        "\n",
        "### Links \n",
        "\n",
        "- https://github.com/Azure-Samples/Synapse/tree/main/Notebooks/PySpark"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define common PySpark functions used in this Notebook\n",
        "\n",
        "def write_df_to_csv_blob(df, out_csv):\n",
        "\n",
        "    # Azure storage account info\n",
        "    blob_account_name   = 'cjoakimstorage'\n",
        "    blob_container_name = 'retail'\n",
        "    blob_relative_path  = 'demo'\n",
        "    linked_service_name = 'SecondaryAzureBlobStorage'\n",
        "\n",
        "    blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(\n",
        "        linked_service_name)\n",
        "    #print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "        blob_container_name, blob_account_name, blob_relative_path)\n",
        "\n",
        "    spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "        blob_container_name, blob_account_name), blob_sas_token)\n",
        "\n",
        "    csv_path = '{}/{}'.format(wasbs_path, out_csv)\n",
        "\n",
        "    print('wasbs_path: ' + wasbs_path)\n",
        "    print('csv_path:   ' + csv_path)\n",
        "\n",
        "    # Write to blob storage, coalesce it into one CSV file\n",
        "    df.coalesce(1).write.csv(csv_path, mode='overwrite', header='true')\n",
        "    print('written')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SynapseLink Sales data into a Dataframe.\n",
        "# Select just the \"sale\" document types from the sales container, \n",
        "# which have a minimum _ts (timestamp) value.\n",
        "\n",
        "# The documents in CosmosDB OLTP data look like this:\n",
        "# { \n",
        "#     \"_id\" : ObjectId(\"6200059edbf78e1f05346e70\"), \n",
        "#     \"pk\" : \"1\", \n",
        "#     \"id\" : \"d6167c84-024a-4ecd-9c95-55f0005615d0\", \n",
        "#     \"sale_id\" : NumberInt(1), \n",
        "#     \"doctype\" : \"sale\", \n",
        "#     \"date\" : \"2021-01-01\", \n",
        "#     \"dow\" : \"fri\", \n",
        "#     \"customer_id\" : NumberInt(3275), \n",
        "#     \"store_id\" : NumberInt(61), \n",
        "#     \"item_count\" : NumberInt(3), \n",
        "#     \"total_cost\" : 2049.71\n",
        "# }\n",
        "\n",
        "# Above document from this query in Studio 3T:\n",
        "#   db.getCollection(\"sales\").find({doctype:\"sale\"})\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# initialize variables\n",
        "min_doc_timestamp = 1635168000\n",
        "\n",
        "# read the raw OLAP data, filtering by document _ts (timestamp)\n",
        "df_sales_raw = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"CosmosMongoDemoDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"sales\")\\\n",
        "    .load().filter(col(\"_ts\") > min_doc_timestamp)\n",
        "\n",
        "display(df_sales_raw.limit(3))\n",
        "df_sales_raw.printSchema()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select just the pertinent columns for the aggregation calculation.\n",
        "# Unpack the struct columns with attrname.* syntax.\n",
        "\n",
        "df_sales_unpacked = df_sales_raw.select(\n",
        "    col('doctype.*'),\n",
        "    col('date.*'),\n",
        "    col('customer_id.*'),\n",
        "    col('item_count.*'),\n",
        "    col('total_cost.*'))\n",
        "\n",
        "display(df_sales_unpacked.limit(3))\n",
        "df_sales_unpacked.printSchema()\n",
        "\n",
        "# Rename the columns of the unpacked DataFrame\n",
        "new_column_names = [\n",
        "    'doctype', 'date', 'customer_id', 'item_count', 'total_cost']\n",
        "df_sales = df_sales_unpacked.toDF(*new_column_names).filter(col(\"doctype\") == \"sale\")\n",
        "\n",
        "display(df_sales.limit(3))\n",
        "df_sales.printSchema()\n",
        "\n",
        "print('df_sales, shape: {} x {}'.format(\n",
        "        df_sales.count(), len(df_sales.columns)))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate Sales by Customer \n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "df_customer_aggregated = df_sales.groupBy(\"customer_id\") \\\n",
        "    .agg(\n",
        "        F.first('customer_id').alias('_id'), \\\n",
        "        F.first('customer_id').alias('pk'), \\\n",
        "        F.count(\"customer_id\").alias('order_count'), \\\n",
        "        F.sum(\"total_cost\").alias(\"total_dollar_amount\"), \\\n",
        "        F.sum(\"item_count\").alias(\"total_item_count\")) \\\n",
        "        .sort(\"customer_id\", ascending=True)\n",
        "\n",
        "display(df_customer_aggregated.limit(4))\n",
        "df_customer_aggregated.printSchema()\n",
        "\n",
        "# Pass the df_customer_aggregated DataFrame to Spark/Scala as a TempView\n",
        "df_customer_aggregated.createOrReplaceTempView(\"CustomerAggSales\")\n",
        "\n",
        "print('df_customer_aggregated, shape: {} x {}'.format(\n",
        "        df_customer_aggregated.count(), \n",
        "        len(df_customer_aggregated.columns)))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the df_customer_aggregated to Azure Blob Storage as CSV \n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# Azure storage account info\n",
        "blob_account_name   = 'cjoakimstorage'\n",
        "blob_container_name = 'synapse'\n",
        "blob_relative_path  = 'retail/sales/mongo/'\n",
        "linked_service_name = 'SecondaryAzureBlobStorage'\n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "#print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "# Allow Spark to access from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "    blob_container_name, blob_account_name, blob_relative_path)\n",
        "\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "    blob_container_name, blob_account_name), blob_sas_token)\n",
        "\n",
        "csv_path = '{}{}'.format(wasbs_path,'sales_by_customer_csv')\n",
        "\n",
        "print('wasbs_path: ' + wasbs_path)\n",
        "print('csv_path:   ' + csv_path)\n",
        "\n",
        "# Write to blob storage, coalesce it into one CSV file\n",
        "df_customer_aggregated.coalesce(1).write.csv(csv_path, mode='overwrite', header='true')\n",
        "print('csv data written to azure storage blob')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark \n",
        "\n",
        "// Obtain configuration values from spark.conf\n",
        "val connStr  = spark.conf.get(\"spark.azurepg.jdbc.connstring\")\n",
        "val driver   = spark.conf.get(\"spark.azurepg.jdbc.driver\")\n",
        "val server   = spark.conf.get(\"spark.azurepg.jdbc.server\")\n",
        "val database = spark.conf.get(\"spark.azurepg.jdbc.database\")\n",
        "val table    = \"public.customer_sales\"\n",
        "val user     = spark.conf.get(\"spark.azurepg.jdbc.user\")\n",
        "val password = spark.conf.get(\"spark.azurepg.jdbc.pass\")\n",
        "\n",
        "// Read the temp table into a Dataframe\n",
        "val df_temp_view = spark.read.table(\"CustomerAggSales\")\n",
        "\n",
        "// Using JDBC, write the Dataframe to Azure PostgreSQL\n",
        "df_temp_view.write\n",
        "  .format(\"jdbc\")\n",
        "  .option(\"url\", connStr)\n",
        "  .option(\"driver\", driver)\n",
        "  .option(\"dbtable\", table)\n",
        "  .option(\"user\", user)\n",
        "  .option(\"password\", password)\n",
        "  .mode(\"overwrite\")\n",
        "  .save()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\n",
        "\n",
        "# Obtain the list of Installed Packages in this Spark cluster,\n",
        "# and write it to blob storage.\n",
        "\n",
        "import pkg_resources\n",
        "pkg_list = list()\n",
        "\n",
        "for d in sorted(pkg_resources.working_set):\n",
        "    pkg_list.append([str(d)])\n",
        "\n",
        "sorted_pkg_list = sorted(pkg_list)\n",
        "\n",
        "columns = ['package_and_version']\n",
        "df_pkg = spark.createDataFrame(sorted_pkg_list, columns)\n",
        "\n",
        "\n",
        "display(df_pkg.limit(3))\n",
        "df_pkg.printSchema()\n",
        "\n",
        "write_df_to_csv_blob(df_pkg, 'installed_packages')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark \n",
        "\n",
        "//import spark.implicits._\n",
        "\n",
        "val connStr  = spark.conf.get(\"spark.azurepg.jdbc.connstring\")\n",
        "val driver   = spark.conf.get(\"spark.azurepg.jdbc.driver\")\n",
        "val server   = spark.conf.get(\"spark.azurepg.jdbc.server\")\n",
        "val database = spark.conf.get(\"spark.azurepg.jdbc.database\")\n",
        "val table    = \"public.packages\"\n",
        "val user     = spark.conf.get(\"spark.azurepg.jdbc.user\")\n",
        "val pass     = spark.conf.get(\"spark.azurepg.jdbc.pass\")\n",
        "\n",
        "val df_hardcoded = Seq(\n",
        "  (\"this\"),\n",
        "  (\"that\"),\n",
        "  (\"the other\")\n",
        ").toDF(\"package_and_version\")\n",
        "\n",
        "df_hardcoded.write\n",
        "  .format(\"jdbc\")\n",
        "  .option(\"url\", connStr)\n",
        "  .option(\"driver\", driver)\n",
        "  .option(\"dbtable\", table)\n",
        "  .option(\"user\", user)\n",
        "  .option(\"password\", pass)\n",
        "  .mode(\"overwrite\")\n",
        "  .save()\n",
        "\n",
        "println(\"df written to database: \" + database + \", table: \" + table)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}