{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Read from Cosmos DB analytical store into a Spark DataFrame and display 10 rows from the DataFrame\n",
        "# To select a preferred list of regions in a multi-region Cosmos DB account, add .option(\"spark.cosmos.preferredRegions\", \"<Region1>,<Region2>\")\n",
        "\n",
        "df = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"CosmosDB_SL_Demo\")\\\n",
        "    .option(\"spark.cosmos.container\", \"travel\")\\\n",
        "    .load()\n",
        "\n",
        "display(df.printSchema())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the row and column counts of the Dataframe\n",
        "\n",
        "print((df.count(), len(df.columns)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# unpack the structs of type string into another dataframe, df2\n",
        "df2 = df.select(\n",
        "    col('route'),\n",
        "    col('id'),\n",
        "    col('doc_time'),\n",
        "    col('date'),\n",
        "    col('count'),\n",
        "    col('to_airport_country'),\n",
        "    col('to_airport_name')).filter(\"_ts > 1630355233\") \n",
        "\n",
        "# rename the unpacked columns, into new dataframe df3\n",
        "new_column_names = ['route','id','doc_time','date','count','to_airport_country','to_airport_name']\n",
        "df3 = df2.toDF(*new_column_names)\n",
        "\n",
        "# create new df4, filtering by route 'ATL:MBJ', sorting by 'doc_time' descending\n",
        "df4 = df3.filter(\"route == 'MIA:MAO'\").sort(\"doc_time\", ascending=False)\n",
        "\n",
        "# display the first 10 rows\n",
        "df4.show(n=20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}