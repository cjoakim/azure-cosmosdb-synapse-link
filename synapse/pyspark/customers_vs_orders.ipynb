{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Process the Customers vs Orders Synapse Link Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SynapseLink Customers and Orders data into a Dataframes.\n",
        "# Then, filter the Orders for just the 'order' doctype.\n",
        "\n",
        "df_customers = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"demoCosmosDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"customers\")\\\n",
        "    .load()\n",
        "\n",
        "df_orders = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"demoCosmosDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"orders\")\\\n",
        "    .load()\n",
        "\n",
        "df_order_docs = df_orders.filter(df_orders[\"doctype\"].isin([\"order\"]))\n",
        "\n",
        "print('df_customers, shape: {} x {}'.format(\n",
        "        df_customers.count(), len(df_customers.columns)))\n",
        "df_customers.printSchema()\n",
        "\n",
        "print('df_orders, shape: {} x {}'.format(\n",
        "        df_orders.count(), len(df_orders.columns)))\n",
        "df_orders.printSchema()\n",
        "\n",
        "print('df_order_docs, shape: {} x {}'.format(\n",
        "        df_order_docs.count(), len(df_order_docs.columns)))\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 66,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:27.9283121Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:00.9732615Z",
              "execution_finish_time": "2021-10-19T20:14:06.6863205Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 66, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_customers, shape: 100000 x 13\nroot\n |-- _rid: string (nullable = true)\n |-- _ts: long (nullable = true)\n |-- pk: string (nullable = true)\n |-- doctype: string (nullable = true)\n |-- customerId: string (nullable = true)\n |-- name: string (nullable = true)\n |-- first: string (nullable = true)\n |-- last: string (nullable = true)\n |-- address: struct (nullable = true)\n |    |-- street: string (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |    |-- zip: string (nullable = true)\n |-- doc_epoch: long (nullable = true)\n |-- doc_time: string (nullable = true)\n |-- id: string (nullable = true)\n |-- _etag: string (nullable = true)\n\ndf_orders, shape: 1049182 x 21\nroot\n |-- _rid: string (nullable = true)\n |-- _ts: long (nullable = true)\n |-- pk: long (nullable = true)\n |-- doctype: string (nullable = true)\n |-- orderId: long (nullable = true)\n |-- lineNumber: long (nullable = true)\n |-- customerId: string (nullable = true)\n |-- sku: long (nullable = true)\n |-- name: string (nullable = true)\n |-- qty: long (nullable = true)\n |-- price: double (nullable = true)\n |-- item_total: double (nullable = true)\n |-- doc_epoch: long (nullable = true)\n |-- doc_time: string (nullable = true)\n |-- id: string (nullable = true)\n |-- _etag: string (nullable = true)\n |-- status: string (nullable = true)\n |-- date_time: string (nullable = true)\n |-- item_count: long (nullable = true)\n |-- order_total: double (nullable = true)\n |-- delivery_count: long (nullable = true)\n\ndf_order_docs, shape: 300000 x 21"
          ]
        }
      ],
      "execution_count": 67,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the df_customers Dataframe\n",
        "\n",
        "display(df_customers.limit(3))\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 67,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:27.9825517Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:06.7831318Z",
              "execution_finish_time": "2021-10-19T20:14:07.8932603Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 67, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "ac537255-a277-457b-9c12-fd990df55078",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, ac537255-a277-457b-9c12-fd990df55078)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 68,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the df_order_docs Dataframe\n",
        "\n",
        "display(df_order_docs.limit(3))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 68,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:28.0363599Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:08.3095212Z",
              "execution_finish_time": "2021-10-19T20:14:11.1110383Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 68, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "6dd9a236-649b-4e52-901a-c4be58c06e15",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 6dd9a236-649b-4e52-901a-c4be58c06e15)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 69,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Narrower/Minimal Dataframes for the Join operation \n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_customers_minimal = df_customers.select(\n",
        "    col('customerId'),\n",
        "    col('name'))\n",
        "\n",
        "print('df_customers_minimal, shape: {} x {}'.format(\n",
        "        df_customers_minimal.count(), len(df_customers_minimal.columns)))\n",
        "df_customers_minimal.printSchema()\n",
        "\n",
        "df_orders_minimal = df_order_docs.select(\n",
        "    col('orderId'),\n",
        "    col('customerId'),\n",
        "    col('item_count'),\n",
        "    col('order_total'))\n",
        "\n",
        "print('df_orders_minimal, shape: {} x {}'.format(\n",
        "        df_orders_minimal.count(), len(df_orders_minimal.columns)))\n",
        "df_orders_minimal.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 69,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:28.0914275Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:12.2277066Z",
              "execution_finish_time": "2021-10-19T20:14:13.6794901Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 69, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_customers_minimal, shape: 100000 x 2\nroot\n |-- customerId: string (nullable = true)\n |-- name: string (nullable = true)\n\ndf_orders_minimal, shape: 300000 x 4\nroot\n |-- orderId: long (nullable = true)\n |-- customerId: string (nullable = true)\n |-- item_count: long (nullable = true)\n |-- order_total: double (nullable = true)"
          ]
        }
      ],
      "execution_count": 70,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the (narrow) Customers to their (narrow) Order documents\n",
        "\n",
        "df_joined = df_orders_minimal.join(df_customers_minimal, ['customerId']) \\\n",
        "    .sort(\"customerId\", ascending=False)\n",
        "\n",
        "\n",
        "print('df_joined, shape: {} x {}'.format(\n",
        "        df_joined.count(), len(df_joined.columns)))\n",
        "df_joined.printSchema()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 70,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:28.1759483Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:14.0272232Z",
              "execution_finish_time": "2021-10-19T20:14:16.9898021Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 70, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_joined, shape: 300000 x 5\nroot\n |-- customerId: string (nullable = true)\n |-- orderId: long (nullable = true)\n |-- item_count: long (nullable = true)\n |-- order_total: double (nullable = true)\n |-- name: string (nullable = true)"
          ]
        }
      ],
      "execution_count": 71,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the df_joined Dataframe\n",
        "\n",
        "display(df_joined.limit(20))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 71,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:28.2325118Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:17.5776794Z",
              "execution_finish_time": "2021-10-19T20:14:20.703287Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 71, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "f54caf6d-cc3d-40bd-9f4c-6f8da926da01",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, f54caf6d-cc3d-40bd-9f4c-6f8da926da01)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 72,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the df_joined Dataframe by customerId, sum on order total and total_orders\n",
        "\n",
        "df_grouped = df_joined.groupby(\"customerId\") \\\n",
        "    .sum(\"order_total\").alias('total_orders') \\\n",
        "    .sort(\"customerId\", ascending=False)\n",
        "\n",
        "display(df_grouped.printSchema())\n",
        "print((df_grouped.count(), len(df_grouped.columns)))\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 72,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:28.2935435Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:22.0751947Z",
              "execution_finish_time": "2021-10-19T20:14:27.1347265Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 72, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- customerId: string (nullable = true)\n |-- sum(order_total): double (nullable = true)\n\n(95119, 2)"
          ]
        }
      ],
      "execution_count": 73,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F \n",
        "\n",
        "df_agg = df_joined.groupBy(\"customerId\") \\\n",
        "    .agg(\n",
        "        F.count(\"customerId\").alias('order_count'), \\\n",
        "        F.sum(\"order_total\").alias(\"total_dollar_amount\"), \\\n",
        "        F.sum(\"item_count\").alias(\"total_item_count\")) \\\n",
        "        .sort(\"customerId\", ascending=False)\n",
        "\n",
        "display(df_agg.printSchema())\n",
        "print((df_agg.count(), len(df_agg.columns)))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 73,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:28.3617827Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:27.2436542Z",
              "execution_finish_time": "2021-10-19T20:14:31.9347378Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 73, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- customerId: string (nullable = true)\n |-- order_count: long (nullable = false)\n |-- total_dollar_amount: double (nullable = true)\n |-- total_item_count: long (nullable = true)\n\n(95119, 4)"
          ]
        }
      ],
      "execution_count": 74,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_agg.limit(30))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 74,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:13:28.4124657Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:14:32.2597998Z",
              "execution_finish_time": "2021-10-19T20:14:38.6701459Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 74, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "78f60f87-0100-42a8-9b44-c0a9d55ae228",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 78f60f87-0100-42a8-9b44-c0a9d55ae228)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# See https://github.com/Azure-Samples/Synapse/blob/main/Notebooks/PySpark/02%20Read%20and%20write%20data%20from%20Azure%20Blob%20Storage%20WASB.ipynb\n",
        "\n",
        "# Azure storage access info\n",
        "blob_account_name   = 'cjoakimstorage'\n",
        "blob_container_name = 'synapse'\n",
        "blob_relative_path  = 'ecomm/'\n",
        "linked_service_name = 'cjoakimstorageAzureBlobStorage'\n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(\n",
        "    linked_service_name)\n",
        "print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "# Allow Spark to access from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "    blob_container_name, blob_account_name, blob_relative_path)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "    blob_container_name, blob_account_name), blob_sas_token)\n",
        "print('Remote blob path: ' + wasbs_path)\n",
        "\n",
        "csv_path  = '{}{}'.format(wasbs_path,'sales_by_customer_csv')\n",
        "json_path = '{}{}'.format(wasbs_path,'sales_by_customer_json')\n",
        "\n",
        "#df_agg.coalesce(1).write.csv(blob_path, mode='overwrite', header='true')\n",
        "df_agg.coalesce(1).write.json(blob_path, mode='overwrite')\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 81,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T20:47:55.4680408Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T20:47:55.6052419Z",
              "execution_finish_time": "2021-10-19T20:48:02.5396863Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 81, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blob_sas_token: ?sv=2020-02-10&ss=bf&srt=sco&se=2021-10-20T20%3A10%3A29Z&sp=rwdl&sig=sdEc8zZLuIxjJ1Yu%2BPjRxbQl3Bl5xXJiKBoeXuvc5hU%3D\nRemote blob path: wasbs://synapse@cjoakimstorage.blob.core.windows.net/ecomm/"
          ]
        }
      ],
      "execution_count": 82,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write to CosmosDB - linked service 'demoCosmosDB'\n",
        "# See https://docs.microsoft.com/en-us/azure/synapse-analytics/synapse-link/how-to-query-analytical-store-spark#write-spark-dataframe-to-azure-cosmos-db-container\n",
        "\n",
        "df_agg.write.format(\"cosmos.oltp\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"demoCosmosDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"sales_by_customer\")\\\n",
        "    .option(\"spak.cosmos.write.upsertenabled\", \"true\")\\\n",
        "    .mode('append')\\\n",
        "    .save()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "poolspark3s",
              "session_id": 43,
              "statement_id": 89,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-19T21:18:19.6158219Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-19T21:18:19.7340207Z",
              "execution_finish_time": "2021-10-19T21:18:28.3101756Z"
            },
            "text/plain": "StatementMeta(poolspark3s, 43, 89, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o865.save.\n: org.apache.spark.SparkException: Writing job aborted.\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:133)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:132)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:995)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:995)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:371)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:302)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 287.0 failed 4 times, most recent failure: Lost task 0.3 in stage 287.0 (TID 3748) (5b2f0c58f6614a68960869eda29e1f1200159367221 executor 1): java.lang.IllegalArgumentException: requirement failed: id is a mandatory field. But it is missing or it is not a string. Json: {\"customerId\":\"0099997114579\",\"order_count\":4,\"total_dollar_amount\":1047.5700000000002,\"total_item_count\":7}\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:96)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:69)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:416)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: The Spark task was aborted, Context: SparkTaskContext(correlationActivityId=a3b32723-c63d-4a69-9cbe-aeab6b376c79,stageId=287,partitionId=0,taskAttemptId=3748,details=)\n\t\tat com.azure.cosmos.spark.BulkWriter.abort(BulkWriter.scala:484)\n\t\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.abort(ItemsDataWriteFactory.scala:119)\n\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:448)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1484)\n\t\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2263)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2212)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2211)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1082)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1082)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1082)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2392)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2381)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:869)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n\t... 33 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: id is a mandatory field. But it is missing or it is not a string. Json: {\"customerId\":\"0099997114579\",\"order_count\":4,\"total_dollar_amount\":1047.5700000000002,\"total_item_count\":7}\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:96)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:69)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:416)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.lang.IllegalStateException: The Spark task was aborted, Context: SparkTaskContext(correlationActivityId=a3b32723-c63d-4a69-9cbe-aeab6b376c79,stageId=287,partitionId=0,taskAttemptId=3748,details=)\n\t\tat com.azure.cosmos.spark.BulkWriter.abort(BulkWriter.scala:484)\n\t\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.abort(ItemsDataWriteFactory.scala:119)\n\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:448)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1484)\n\t\t... 10 more\n",
          "traceback": [
            "Py4JJavaError: An error occurred while calling o865.save.\n: org.apache.spark.SparkException: Writing job aborted.\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:133)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:132)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:995)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:995)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:371)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:302)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 287.0 failed 4 times, most recent failure: Lost task 0.3 in stage 287.0 (TID 3748) (5b2f0c58f6614a68960869eda29e1f1200159367221 executor 1): java.lang.IllegalArgumentException: requirement failed: id is a mandatory field. But it is missing or it is not a string. Json: {\"customerId\":\"0099997114579\",\"order_count\":4,\"total_dollar_amount\":1047.5700000000002,\"total_item_count\":7}\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:96)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:69)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:416)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: The Spark task was aborted, Context: SparkTaskContext(correlationActivityId=a3b32723-c63d-4a69-9cbe-aeab6b376c79,stageId=287,partitionId=0,taskAttemptId=3748,details=)\n\t\tat com.azure.cosmos.spark.BulkWriter.abort(BulkWriter.scala:484)\n\t\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.abort(ItemsDataWriteFactory.scala:119)\n\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:448)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1484)\n\t\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2263)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2212)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2211)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1082)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1082)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1082)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2392)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2381)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:869)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n\t... 33 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: id is a mandatory field. But it is missing or it is not a string. Json: {\"customerId\":\"0099997114579\",\"order_count\":4,\"total_dollar_amount\":1047.5700000000002,\"total_item_count\":7}\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:96)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:69)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:416)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.lang.IllegalStateException: The Spark task was aborted, Context: SparkTaskContext(correlationActivityId=a3b32723-c63d-4a69-9cbe-aeab6b376c79,stageId=287,partitionId=0,taskAttemptId=3748,details=)\n\t\tat com.azure.cosmos.spark.BulkWriter.abort(BulkWriter.scala:484)\n\t\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.abort(ItemsDataWriteFactory.scala:119)\n\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:448)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1484)\n\t\t... 10 more\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1107, in save\n    self._jwrite.save()\n",
            "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n    return f(*a, **kw)\n",
            "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o865.save.\n: org.apache.spark.SparkException: Writing job aborted.\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:133)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:132)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:995)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:995)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:371)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:302)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 287.0 failed 4 times, most recent failure: Lost task 0.3 in stage 287.0 (TID 3748) (5b2f0c58f6614a68960869eda29e1f1200159367221 executor 1): java.lang.IllegalArgumentException: requirement failed: id is a mandatory field. But it is missing or it is not a string. Json: {\"customerId\":\"0099997114579\",\"order_count\":4,\"total_dollar_amount\":1047.5700000000002,\"total_item_count\":7}\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:96)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:69)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:416)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: The Spark task was aborted, Context: SparkTaskContext(correlationActivityId=a3b32723-c63d-4a69-9cbe-aeab6b376c79,stageId=287,partitionId=0,taskAttemptId=3748,details=)\n\t\tat com.azure.cosmos.spark.BulkWriter.abort(BulkWriter.scala:484)\n\t\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.abort(ItemsDataWriteFactory.scala:119)\n\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:448)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1484)\n\t\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2263)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2212)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2211)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1082)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1082)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1082)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2392)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2381)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:869)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n\t... 33 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: id is a mandatory field. But it is missing or it is not a string. Json: {\"customerId\":\"0099997114579\",\"order_count\":4,\"total_dollar_amount\":1047.5700000000002,\"total_item_count\":7}\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:96)\n\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.write(ItemsDataWriteFactory.scala:69)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:416)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.lang.IllegalStateException: The Spark task was aborted, Context: SparkTaskContext(correlationActivityId=a3b32723-c63d-4a69-9cbe-aeab6b376c79,stageId=287,partitionId=0,taskAttemptId=3748,details=)\n\t\tat com.azure.cosmos.spark.BulkWriter.abort(BulkWriter.scala:484)\n\t\tat com.azure.cosmos.spark.ItemsDataWriteFactory$CosmosWriter.abort(ItemsDataWriteFactory.scala:119)\n\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:448)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1484)\n\t\t... 10 more\n\n"
          ]
        }
      ],
      "execution_count": 90,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {}
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "ac537255-a277-457b-9c12-fd990df55078": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "HfwXAN4KYILhFQAAAAAAAA==",
                  "1": "1633525610",
                  "2": "0045540253226",
                  "3": "customer",
                  "4": "0045540253226",
                  "5": "Sara Bullock",
                  "6": "Sara",
                  "7": "Bullock",
                  "8": {
                    "street": "5540 Gilbert Fort Apt. 631",
                    "city": "New Gloriaberg",
                    "state": "MS",
                    "zip": "68685"
                  },
                  "9": "1633525609614",
                  "10": "2021/10/06-13:06:49",
                  "11": "8608b70a-9a35-4dc2-83f7-38ef506bf846",
                  "12": "\"3606bdbb-0000-0100-0000-615d9f690000\""
                },
                {
                  "0": "HfwXAN4KYILiFQAAAAAAAA==",
                  "1": "1633525610",
                  "2": "0061928492421",
                  "3": "customer",
                  "4": "0061928492421",
                  "5": "Robert Webster",
                  "6": "Robert",
                  "7": "Webster",
                  "8": {
                    "street": "84155 Natasha Parks",
                    "city": "Kruegerfort",
                    "state": "FL",
                    "zip": "46097"
                  },
                  "9": "1633525609615",
                  "10": "2021/10/06-13:06:49",
                  "11": "f45d9316-a5c4-4de1-8c91-44fa2b5f6ad5",
                  "12": "\"3606bebb-0000-0100-0000-615d9f690000\""
                },
                {
                  "0": "HfwXAN4KYILjFQAAAAAAAA==",
                  "1": "1633525610",
                  "2": "0093401313220",
                  "3": "customer",
                  "4": "0093401313220",
                  "5": "Amy Brown",
                  "6": "Amy",
                  "7": "Brown",
                  "8": {
                    "street": "4792 David Heights Suite 405",
                    "city": "Jennifermouth",
                    "state": "IL",
                    "zip": "39502"
                  },
                  "9": "1633525609615",
                  "10": "2021/10/06-13:06:49",
                  "11": "5467feb7-a593-4014-866f-1095d7e18751",
                  "12": "\"3606bfbb-0000-0100-0000-615d9f690000\""
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "_rid",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "_ts",
                  "type": "bigint"
                },
                {
                  "key": "2",
                  "name": "pk",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "doctype",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "customerId",
                  "type": "string"
                },
                {
                  "key": "5",
                  "name": "name",
                  "type": "string"
                },
                {
                  "key": "6",
                  "name": "first",
                  "type": "string"
                },
                {
                  "key": "7",
                  "name": "last",
                  "type": "string"
                },
                {
                  "key": "8",
                  "name": "address",
                  "type": "StructType(StructField(street,StringType,true), StructField(city,StringType,true), StructField(state,StringType,true), StructField(zip,StringType,true))"
                },
                {
                  "key": "9",
                  "name": "doc_epoch",
                  "type": "bigint"
                },
                {
                  "key": "10",
                  "name": "doc_time",
                  "type": "string"
                },
                {
                  "key": "11",
                  "name": "id",
                  "type": "string"
                },
                {
                  "key": "12",
                  "name": "_etag",
                  "type": "string"
                }
              ]
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "1"
                ],
                "isStacked": false
              }
            }
          }
        },
        "6dd9a236-649b-4e52-901a-c4be58c06e15": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "HfwXAItzqDIDAAAAAAAAAA==",
                  "1": "1633525833",
                  "2": "82",
                  "3": "order",
                  "4": "82",
                  "6": "0077294476408",
                  "12": "1633525832230",
                  "13": "2021/10/06-13:10:32",
                  "14": "6541e58a-2786-4c3a-a510-ca3ac9f89433",
                  "15": "\"380697a0-0000-0100-0000-615da0490000\"",
                  "17": "2021-10-05 16:59:17 +00:00",
                  "18": "1",
                  "19": "171.96",
                  "20": "0"
                },
                {
                  "0": "HfwXAItzqDIEAAAAAAAAAA==",
                  "1": "1633525833",
                  "2": "140",
                  "3": "order",
                  "4": "140",
                  "6": "0088587087270",
                  "12": "1633525832238",
                  "13": "2021/10/06-13:10:32",
                  "14": "35c5bdf0-43b9-49a6-aa25-ad7f1f8237b3",
                  "15": "\"380698a0-0000-0100-0000-615da0490000\"",
                  "17": "2021-10-05 16:59:17 +00:00",
                  "18": "2",
                  "19": "3410.64",
                  "20": "0"
                },
                {
                  "0": "HfwXAItzqDIFAAAAAAAAAA==",
                  "1": "1633525833",
                  "2": "83",
                  "3": "order",
                  "4": "83",
                  "6": "0030395879004",
                  "12": "1633525832230",
                  "13": "2021/10/06-13:10:32",
                  "14": "768f578e-e689-4e74-9c53-b645969f72fb",
                  "15": "\"380699a0-0000-0100-0000-615da0490000\"",
                  "17": "2021-10-05 16:59:17 +00:00",
                  "18": "2",
                  "19": "60.95",
                  "20": "0"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "_rid",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "_ts",
                  "type": "bigint"
                },
                {
                  "key": "2",
                  "name": "pk",
                  "type": "bigint"
                },
                {
                  "key": "3",
                  "name": "doctype",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "orderId",
                  "type": "bigint"
                },
                {
                  "key": "5",
                  "name": "lineNumber",
                  "type": "bigint"
                },
                {
                  "key": "6",
                  "name": "customerId",
                  "type": "string"
                },
                {
                  "key": "7",
                  "name": "sku",
                  "type": "bigint"
                },
                {
                  "key": "8",
                  "name": "name",
                  "type": "string"
                },
                {
                  "key": "9",
                  "name": "qty",
                  "type": "bigint"
                },
                {
                  "key": "10",
                  "name": "price",
                  "type": "double"
                },
                {
                  "key": "11",
                  "name": "item_total",
                  "type": "double"
                },
                {
                  "key": "12",
                  "name": "doc_epoch",
                  "type": "bigint"
                },
                {
                  "key": "13",
                  "name": "doc_time",
                  "type": "string"
                },
                {
                  "key": "14",
                  "name": "id",
                  "type": "string"
                },
                {
                  "key": "15",
                  "name": "_etag",
                  "type": "string"
                },
                {
                  "key": "16",
                  "name": "status",
                  "type": "string"
                },
                {
                  "key": "17",
                  "name": "date_time",
                  "type": "string"
                },
                {
                  "key": "18",
                  "name": "item_count",
                  "type": "bigint"
                },
                {
                  "key": "19",
                  "name": "order_total",
                  "type": "double"
                },
                {
                  "key": "20",
                  "name": "delivery_count",
                  "type": "bigint"
                }
              ]
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "1"
                ],
                "isStacked": false
              }
            }
          }
        },
        "f54caf6d-cc3d-40bd-9f4c-6f8da926da01": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "0099997114579",
                  "1": "75945",
                  "2": "3",
                  "3": "278.05",
                  "4": "Lucas Moran"
                },
                {
                  "0": "0099997114579",
                  "1": "166750",
                  "2": "2",
                  "3": "505.36",
                  "4": "Lucas Moran"
                },
                {
                  "0": "0099997114579",
                  "1": "174498",
                  "2": "1",
                  "3": "223.96",
                  "4": "Lucas Moran"
                },
                {
                  "0": "0099997114579",
                  "1": "258532",
                  "2": "1",
                  "3": "40.2",
                  "4": "Lucas Moran"
                },
                {
                  "0": "0099996679260",
                  "1": "179355",
                  "2": "2",
                  "3": "157.79",
                  "4": "Carol Mason"
                },
                {
                  "0": "0099996458490",
                  "1": "129929",
                  "2": "3",
                  "3": "97.93",
                  "4": "Thomas Lloyd"
                },
                {
                  "0": "0099996458490",
                  "1": "137393",
                  "2": "2",
                  "3": "194.98",
                  "4": "Thomas Lloyd"
                },
                {
                  "0": "0099996458490",
                  "1": "159659",
                  "2": "2",
                  "3": "18.99",
                  "4": "Thomas Lloyd"
                },
                {
                  "0": "0099996458490",
                  "1": "252729",
                  "2": "3",
                  "3": "130.71",
                  "4": "Thomas Lloyd"
                },
                {
                  "0": "0099994522674",
                  "1": "269972",
                  "2": "3",
                  "3": "425.58",
                  "4": "David Farmer"
                },
                {
                  "0": "0099994522674",
                  "1": "265033",
                  "2": "1",
                  "3": "95.96",
                  "4": "David Farmer"
                },
                {
                  "0": "0099993481224",
                  "1": "215881",
                  "2": "3",
                  "3": "50.96",
                  "4": "Tara Washington"
                },
                {
                  "0": "0099993481224",
                  "1": "241611",
                  "2": "2",
                  "3": "33.64",
                  "4": "Tara Washington"
                },
                {
                  "0": "0099993171415",
                  "1": "75449",
                  "2": "3",
                  "3": "265.97",
                  "4": "Jeremy Richardson"
                },
                {
                  "0": "0099993171415",
                  "1": "229520",
                  "2": "3",
                  "3": "379.95",
                  "4": "Jeremy Richardson"
                },
                {
                  "0": "0099992237310",
                  "1": "87969",
                  "2": "3",
                  "3": "200.91",
                  "4": "Harry Kim"
                },
                {
                  "0": "0099992237310",
                  "1": "99510",
                  "2": "3",
                  "3": "2151.7",
                  "4": "Harry Kim"
                },
                {
                  "0": "0099992237310",
                  "1": "107168",
                  "2": "1",
                  "3": "69.98",
                  "4": "Harry Kim"
                },
                {
                  "0": "0099992237310",
                  "1": "122988",
                  "2": "2",
                  "3": "1292.38",
                  "4": "Harry Kim"
                },
                {
                  "0": "0099992237310",
                  "1": "142412",
                  "2": "2",
                  "3": "80.82",
                  "4": "Harry Kim"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "customerId",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "orderId",
                  "type": "bigint"
                },
                {
                  "key": "2",
                  "name": "item_count",
                  "type": "bigint"
                },
                {
                  "key": "3",
                  "name": "order_total",
                  "type": "double"
                },
                {
                  "key": "4",
                  "name": "name",
                  "type": "string"
                }
              ]
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "1"
                ],
                "isStacked": false
              }
            }
          }
        },
        "78f60f87-0100-42a8-9b44-c0a9d55ae228": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "0099997114579",
                  "1": "4",
                  "2": "1047.5700000000002",
                  "3": "7"
                },
                {
                  "0": "0099996679260",
                  "1": "1",
                  "2": "157.79",
                  "3": "2"
                },
                {
                  "0": "0099996458490",
                  "1": "4",
                  "2": "442.61",
                  "3": "10"
                },
                {
                  "0": "0099994522674",
                  "1": "2",
                  "2": "521.54",
                  "3": "4"
                },
                {
                  "0": "0099993481224",
                  "1": "2",
                  "2": "84.6",
                  "3": "5"
                },
                {
                  "0": "0099993171415",
                  "1": "2",
                  "2": "645.9200000000001",
                  "3": "6"
                },
                {
                  "0": "0099992237310",
                  "1": "7",
                  "2": "3858.7499999999995",
                  "3": "13"
                },
                {
                  "0": "0099990573281",
                  "1": "4",
                  "2": "1015.97",
                  "3": "6"
                },
                {
                  "0": "0099990444161",
                  "1": "1",
                  "2": "64.32",
                  "3": "3"
                },
                {
                  "0": "0099989163721",
                  "1": "2",
                  "2": "1347.67",
                  "3": "3"
                },
                {
                  "0": "0099987195205",
                  "1": "3",
                  "2": "266.67",
                  "3": "5"
                },
                {
                  "0": "0099986506255",
                  "1": "2",
                  "2": "556.72",
                  "3": "4"
                },
                {
                  "0": "0099985395218",
                  "1": "3",
                  "2": "316.47",
                  "3": "5"
                },
                {
                  "0": "0099984811184",
                  "1": "3",
                  "2": "320.86",
                  "3": "6"
                },
                {
                  "0": "0099984373828",
                  "1": "3",
                  "2": "750.11",
                  "3": "7"
                },
                {
                  "0": "0099984200674",
                  "1": "3",
                  "2": "846.77",
                  "3": "5"
                },
                {
                  "0": "0099984106600",
                  "1": "4",
                  "2": "1802.58",
                  "3": "9"
                },
                {
                  "0": "0099982998825",
                  "1": "4",
                  "2": "2820.35",
                  "3": "8"
                },
                {
                  "0": "0099982104301",
                  "1": "3",
                  "2": "833.64",
                  "3": "5"
                },
                {
                  "0": "0099978108399",
                  "1": "1",
                  "2": "89.55",
                  "3": "2"
                },
                {
                  "0": "0099976447582",
                  "1": "2",
                  "2": "493.51",
                  "3": "6"
                },
                {
                  "0": "0099976438818",
                  "1": "4",
                  "2": "1156.5",
                  "3": "9"
                },
                {
                  "0": "0099974596244",
                  "1": "3",
                  "2": "1179.73",
                  "3": "8"
                },
                {
                  "0": "0099974500487",
                  "1": "2",
                  "2": "304.87",
                  "3": "4"
                },
                {
                  "0": "0099972873040",
                  "1": "3",
                  "2": "1207.87",
                  "3": "4"
                },
                {
                  "0": "0099971391217",
                  "1": "4",
                  "2": "1038.25",
                  "3": "10"
                },
                {
                  "0": "0099970744250",
                  "1": "3",
                  "2": "436.61",
                  "3": "4"
                },
                {
                  "0": "0099970444105",
                  "1": "3",
                  "2": "622.74",
                  "3": "7"
                },
                {
                  "0": "0099970124182",
                  "1": "2",
                  "2": "443.88",
                  "3": "4"
                },
                {
                  "0": "0099970084738",
                  "1": "1",
                  "2": "493.72",
                  "3": "3"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "customerId",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "order_count",
                  "type": "bigint"
                },
                {
                  "key": "2",
                  "name": "total_dollar_amount",
                  "type": "double"
                },
                {
                  "key": "3",
                  "name": "total_item_count",
                  "type": "bigint"
                }
              ]
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "1"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}