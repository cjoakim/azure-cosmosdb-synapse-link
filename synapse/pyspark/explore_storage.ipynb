{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks?tabs=classical#bring-data-to-a-notebook\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Azure Storage account and blob info:\n",
        "blob_account_name = 'cjoakimstorage' # replace with your blob name\n",
        "blob_container_name = 'synapse' # replace with your container name\n",
        "blob_relative_path = 'usa-states.csv' # replace with your relative folder path\n",
        "linked_service_name = 'cjoakimstorageAzureBlobStorage'\n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\n",
        "print('wasb_path: {}'.format(wasb_path))\n",
        "\n",
        "df = spark.read.option(\"header\", \"true\") \\\n",
        "            .option(\"delimiter\",\"|\") \\\n",
        "            .csv(wasb_path)\n",
        "\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark.conf.set(\"my.number\", 42)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = spark.conf.get(\"my.number\")\n",
        "print(n)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Azure Storage account and blob info:\n",
        "blob_account_name = 'cjoakimstorage' # replace with your blob name\n",
        "blob_container_name = 'synapse' # replace with your container name\n",
        "blob_relative_path = 'usa-states-mod.csv' # replace with your relative folder path\n",
        "linked_service_name = 'cjoakimstorageAzureBlobStorage'\n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "print('wasb_path: {}'.format(wasb_path))\n",
        "\n",
        "df.coalesce(1).write.csv(wasb_path, mode='overwrite', header='true')\n",
        "print('df written to blob storage')\n",
        "\n",
        "df2 = spark.read.option(\"header\", \"true\") \\\n",
        "            .option(\"delimiter\",\",\") \\\n",
        "            .csv(wasb_path)\n",
        "\n",
        "display(df2.limit(100))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}