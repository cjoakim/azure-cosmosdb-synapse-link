{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Load the SynapseLink Customers and Sales SynapseLink Data into a Dataframes.\n",
        "# Select just the \"sale\" document types from the sales container, which have a\n",
        "# minimum _ts (timestamp) value\n",
        "\n",
        "# The documents in CosmosDB OLTP data look like this:\n",
        "# {\n",
        "#   \"pk\": 28923,\n",
        "#   \"id\": \"8500eb9c-390c-462b-87ce-5fb5b8f6359a\",\n",
        "#   \"sale_id\": 28923,\n",
        "#   \"doctype\": \"sale\",\n",
        "#   \"date\": \"2022-01-25\",\n",
        "#   \"dow\": \"tue\",\n",
        "#   \"customer_id\": 1928,\n",
        "#   \"store_id\": 2,\n",
        "#   \"item_count\": 3,\n",
        "#   \"total_cost\": 4566.03\n",
        "# }\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# initialize variables\n",
        "min_doc_timestamp = 1635168000\n",
        "\n",
        "# read the raw OLAP data, filtering by document _ts (timestamp)\n",
        "df_sales_raw = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"CosmosMongoDemoDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"sales\")\\\n",
        "    .load().filter(col(\"_ts\") > min_doc_timestamp)\n",
        "\n",
        "display(df_sales_raw.limit(3))\n",
        "df_sales_raw.printSchema()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select just the pertinet columns.\n",
        "# Unpack the struct columns with attrname.* syntax.\n",
        "# Filter by doctype == 'sale'\n",
        "\n",
        "df_sales_unpacked = df_sales_raw.select(\n",
        "    col('doctype.*'),\n",
        "    col('date.*'),\n",
        "    col('customer_id.*'),\n",
        "    col('item_count.*'),\n",
        "    col('total_cost.*'))\n",
        "\n",
        "display(df_sales_unpacked.limit(3))\n",
        "df_sales_unpacked.printSchema()\n",
        "\n",
        "# Rename the columns of the unpacked DataFrame\n",
        "new_column_names = ['doctype', 'date', 'customer_id', 'item_count', 'total_cost']\n",
        "df_sales = df_sales_unpacked.toDF(*new_column_names).filter(col(\"doctype\") == \"sale\")\n",
        "\n",
        "display(df_sales.limit(3))\n",
        "df_sales.printSchema()\n",
        "\n",
        "print('df_sales, shape: {} x {}'.format(\n",
        "        df_sales.count(), len(df_sales.columns)))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate Sales by Customer \n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "df_customer_aggregated = df_sales.groupBy(\"customer_id\") \\\n",
        "    .agg(\n",
        "        F.first('customer_id').alias('_id'), \\\n",
        "        F.first('customer_id').alias('pk'), \\\n",
        "        F.count(\"customer_id\").alias('order_count'), \\\n",
        "        F.sum(\"total_cost\").alias(\"total_dollar_amount\"), \\\n",
        "        F.sum(\"item_count\").alias(\"total_item_count\")) \\\n",
        "        .sort(\"customer_id\", ascending=True)\n",
        "\n",
        "display(df_customer_aggregated.limit(4))\n",
        "df_customer_aggregated.printSchema()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the df_customer_aggregated to Azure Blob Storage as CSV \n",
        "# See https://github.com/Azure-Samples/Synapse/tree/main/Notebooks/PySpark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# Azure storage account info\n",
        "blob_account_name   = 'cjoakimstorage'\n",
        "blob_container_name = 'synapse'\n",
        "blob_relative_path  = 'retail/sales/mongo/'\n",
        "linked_service_name = 'SecondaryAzureBlobStorage'\n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "#print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "# Allow Spark to access from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "    blob_container_name, blob_account_name, blob_relative_path)\n",
        "\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "    blob_container_name, blob_account_name), blob_sas_token)\n",
        "\n",
        "csv_path = '{}{}'.format(wasbs_path,'sales_by_customer_csv')\n",
        "\n",
        "print('wasbs_path: ' + wasbs_path)\n",
        "print('csv_path:   ' + csv_path)\n",
        "\n",
        "# Write to blob storage, coalesce it into one CSV file\n",
        "df_customer_aggregated.coalesce(1).write.csv(csv_path, mode='overwrite', header='true')\n",
        "print('csv data written to azure storage blob')\n",
        "\n",
        "#df_agg.coalesce(1).write.json(json_path, mode='overwrite')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}