{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\n",
        "\n",
        "from pyspark.sql.types import * \n",
        "from graphframes import *\n",
        "\n",
        "blob_account_name = \"cjoakimstorage\"\n",
        "blob_container_name = \"synapse\"\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "blob_sas_token = token_library.getConnectionString(\"cjoakimstorageAzureBlobStorage\")\n",
        "\n",
        "vertices_csv_blob = 'wasbs://synapse@cjoakimstorage.blob.core.windows.net/graphframes/imdb_vertices.csv'\n",
        "edges_csv_blob    = 'wasbs://synapse@cjoakimstorage.blob.core.windows.net/graphframes/imdb_edges.csv'\n",
        "\n",
        "spark.conf.set(\n",
        "    'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
        "    blob_sas_token)\n",
        "\n",
        "v_fields = [\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"label\", StringType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"attributes\", StringType(), True)\n",
        "]\n",
        "\n",
        "e_fields = [\n",
        "    StructField(\"src\", StringType(), True),\n",
        "    StructField(\"dst\", StringType(), True),\n",
        "    StructField(\"relationship\", StringType(), True),\n",
        "    StructField(\"attributes\", StringType(), True)\n",
        "]\n",
        "\n",
        "df_v = spark.read.load(\n",
        "    vertices_csv_blob, \n",
        "    format='csv', \n",
        "    header=True, \n",
        "    delimiter='|',\n",
        "    schema=StructType(v_fields))\n",
        "\n",
        "df_e = spark.read.load(\n",
        "    edges_csv_blob,\n",
        "    format='csv',\n",
        "    header=True,\n",
        "    delimiter='|',\n",
        "    schema=StructType(e_fields))\n",
        "\n",
        "print('dv_v')\n",
        "print(str(type(df_v)))  # <class 'pyspark.sql.dataframe.DataFrame'>\n",
        "df_v.printSchema()\n",
        "display(df_v.limit(10))\n",
        "\n",
        "\n",
        "print('dv_e')\n",
        "print(str(type(df_e)))\n",
        "df_e.printSchema()\n",
        "display(df_e.limit(10))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "def write_df_to_csv_blob(df, out_csv):\n",
        "    # See https://github.com/Azure-Samples/Synapse/tree/main/Notebooks/PySpark\n",
        "\n",
        "    # Azure storage account info\n",
        "    blob_account_name   = 'cjoakimstorage'\n",
        "    blob_container_name = 'synapse'\n",
        "    blob_relative_path  = 'graphframes'\n",
        "    linked_service_name = 'cjoakimstorageAzureBlobStorage'\n",
        "\n",
        "    blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "    #print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "    # Allow Spark to access from Blob remotely\n",
        "    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "        blob_container_name, blob_account_name, blob_relative_path)\n",
        "\n",
        "    spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "        blob_container_name, blob_account_name), blob_sas_token)\n",
        "\n",
        "    csv_path = '{}{}'.format(wasbs_path, out_csv)\n",
        "\n",
        "    print('wasbs_path: ' + wasbs_path)\n",
        "    print('csv_path:   ' + csv_path)\n",
        "\n",
        "    # Write to blob storage, coalesce it into one CSV file\n",
        "    df.coalesce(1).write.csv(csv_path, mode='overwrite', header='true')\n",
        "    print('written')\n",
        "\n",
        "def write_df_to_json_blob(df, out):\n",
        "    # See https://github.com/Azure-Samples/Synapse/tree/main/Notebooks/PySpark\n",
        "\n",
        "    # Azure storage account info\n",
        "    blob_account_name   = 'cjoakimstorage'\n",
        "    blob_container_name = 'synapse'\n",
        "    blob_relative_path  = 'graphframes'\n",
        "    linked_service_name = 'cjoakimstorageAzureBlobStorage'\n",
        "\n",
        "    blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "    #print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "    # Allow Spark to access from Blob remotely\n",
        "    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "        blob_container_name, blob_account_name, blob_relative_path)\n",
        "\n",
        "    spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "        blob_container_name, blob_account_name), blob_sas_token)\n",
        "\n",
        "    out_path = '{}{}'.format(wasbs_path, out)\n",
        "\n",
        "    print('wasbs_path: ' + wasbs_path)\n",
        "    print('out_path:   ' + out_path)\n",
        "\n",
        "    # Write to blob storage, coalesce it into one file\n",
        "    df.coalesce(1).write.json(out_path, mode='overwrite')\n",
        "    print('written')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\n",
        "\n",
        "# https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks\n",
        "\n",
        "df_v.createOrReplaceTempView('dfv')\n",
        "print('df_v cached')\n",
        "\n",
        "df_e.createOrReplaceTempView('dfe')\n",
        "print('df_e cached')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "select * from dfv limit 3\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "select * from dfe limit 3"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "val df_v = spark.sql(\"select * from dfv\")\n",
        "df_v.printSchema()\n",
        "\n",
        "\n",
        "val df_e = spark.sql(\"select * from dfe\")\n",
        "df_e.printSchema()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark \n",
        "\n",
        "# Create the GraphFrame, g, from the Vertices DataFrame and Edges DataFrame\n",
        "\n",
        "from graphframes import *\n",
        "\n",
        "g = GraphFrame(df_v, df_e)\n",
        "print('done')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inDegrees\n",
        "\n",
        "display(g.inDegrees)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search the paths from Kevin Bacon (nm0000102) to Charlotte Rampling (nm0001648).\n",
        "\n",
        "paths = g.bfs(\"id = 'nm0000102'\", \"id = 'nm0001648'\")\n",
        "paths.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "\n",
        "total_degree = g.degrees\n",
        "in_degree = g.inDegrees\n",
        "out_degree = g.outDegrees\n",
        "\n",
        "result = (total_degree.join(in_degree, \"id\", how=\"left\")\n",
        "    .join(out_degree, \"id\", how=\"left\")\n",
        "    .fillna(0)\n",
        "    .sort(\"inDegree\", ascending=False))\n",
        "\n",
        "print(str(type(result)))  # <class 'pyspark.sql.dataframe.DataFrame'>\n",
        "result.show()\n",
        "\n",
        "write_df_to_csv_blob(result, 'degrees')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search the paths from Kevin Bacon (nm0000102) to Charlotte Rampling (nm0001648).\n",
        "\n",
        "result = g.bfs(\"id = 'nm0000102'\", \"id = 'nm0001648'\")\n",
        "result.show()\n",
        "\n",
        "print(str(type(result)))  # <class 'pyspark.sql.dataframe.DataFrame'>\n",
        "\n",
        "write_df_to_json_blob(result, 'paths_bacon_rampling')\n",
        "\n",
        "\n",
        "# CSV data source does not support struct<id:string,label:string,name:string,attributes:string> data type.\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search the Breadth First Search (bfs) paths from Lori Singer (nm0001742) to Charlotte Rampling (nm0001648).\n",
        "\n",
        "paths = g.bfs(\"id = 'nm0001742'\", \"id = 'nm0001648'\")\n",
        "paths.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search the paths from Kevin Bacon (nm0001742) to Charlotte Rampling (nm0001648).\n",
        "\n",
        "paths = g.shortestPaths(['nm0000102','nm0001648'])\n",
        "paths.show()\n",
        "\n",
        "write_df_to_csv_blob(paths, 'shortest_paths.csv')\n",
        "\n",
        "#  CSV data source does not support map<string,int> data type\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}