{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Process the Customers vs Orders Synapse Link Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SynapseLink Customers and Orders SynapseLink Data into a Dataframes.\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# initialize variables\n",
        "use_push_down_predicate = True\n",
        "df_orders, df_order_docs = None, None\n",
        "min_timestamp = 1635168000\n",
        "\n",
        "# read the customers SynapseLink data\n",
        "df_customers = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"demoCosmosDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"customers\")\\\n",
        "    .load().filter(col(\"_ts\") > min_timestamp)\n",
        "\n",
        "\n",
        "if use_push_down_predicate == True:\n",
        "    # this is more efficient, as the dataframe is filtered on load\n",
        "    df_order_docs = spark.read\\\n",
        "        .format(\"cosmos.olap\")\\\n",
        "        .option(\"spark.synapse.linkedService\", \"demoCosmosDB\")\\\n",
        "        .option(\"spark.cosmos.container\", \"orders\")\\\n",
        "        .load().filter(col(\"doctype\") == \"order\").filter(col(\"_ts\") > min_timestamp) \n",
        "        # push-down predicate filters on doctype and _ts \n",
        "else:\n",
        "    # this is less efficient, as the dataframe is filtered after load\n",
        "    df_orders = spark.read\\\n",
        "        .format(\"cosmos.olap\")\\\n",
        "        .option(\"spark.synapse.linkedService\", \"demoCosmosDB\")\\\n",
        "        .option(\"spark.cosmos.container\", \"orders\")\\\n",
        "        .load()\n",
        "\n",
        "    print('df_orders, shape: {} x {}'.format(\n",
        "        df_orders.count(), len(df_orders.columns)))\n",
        "    df_orders.printSchema()\n",
        "\n",
        "    df_order_docs = df_orders.filter(df_orders[\"doctype\"].isin([\"order\"]))\n",
        "\n",
        "print('df_customers, shape: {} x {}'.format(\n",
        "        df_customers.count(), len(df_customers.columns)))\n",
        "df_customers.printSchema()\n",
        "\n",
        "print('df_order_docs, shape: {} x {}'.format(\n",
        "        df_order_docs.count(), len(df_order_docs.columns)))\n",
        "\n",
        "# df_orders, shape: 4199812 x 26\n",
        "# df_order_docs, shape: 1200000 x 26\n",
        "# df_order_docs, shape: 300000 x 26"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the df_customers Dataframe\n",
        "\n",
        "display(df_customers.limit(3))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the df_order_docs Dataframe\n",
        "\n",
        "display(df_order_docs.limit(3))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Narrower/Minimal Dataframes for the Join operation \n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_customers_minimal = df_customers.select(\n",
        "    col('id'),\n",
        "    col('customer_id'),\n",
        "    col('name'))\n",
        "\n",
        "print('df_customers_minimal, shape: {} x {}'.format(\n",
        "        df_customers_minimal.count(), len(df_customers_minimal.columns)))\n",
        "df_customers_minimal.printSchema()\n",
        "\n",
        "df_orders_minimal = df_order_docs.select(\n",
        "    col('order_id'),\n",
        "    col('customer_id'),\n",
        "    col('item_count'),\n",
        "    col('order_total'))\n",
        "\n",
        "print('df_orders_minimal, shape: {} x {}'.format(\n",
        "        df_orders_minimal.count(), len(df_orders_minimal.columns)))\n",
        "df_orders_minimal.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the (narrow) Customers to their (narrow) Order documents\n",
        "\n",
        "df_joined = df_orders_minimal.join(df_customers_minimal, ['customer_id']) \\\n",
        "    .sort(\"customer_id\", ascending=False)\n",
        "\n",
        "\n",
        "print('df_joined, shape: {} x {}'.format(\n",
        "        df_joined.count(), len(df_joined.columns)))\n",
        "df_joined.printSchema()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the df_joined Dataframe\n",
        "\n",
        "display(df_joined.limit(20))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the df_joined Dataframe by customerId, sum on order total and total_orders\n",
        "\n",
        "df_grouped = df_joined.groupby(\"customer_id\") \\\n",
        "    .sum(\"order_total\").alias('total_orders') \\\n",
        "    .sort(\"customer_id\", ascending=False)\n",
        "\n",
        "display(df_grouped.printSchema())\n",
        "print((df_grouped.count(), len(df_grouped.columns)))\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F \n",
        "#from pyspark.sql.functions import col\n",
        "\n",
        "df_agg = df_joined.groupBy(\"customer_id\") \\\n",
        "    .agg(\n",
        "        F.first('id').alias('id'), \\\n",
        "        F.count(\"customer_id\").alias('order_count'), \\\n",
        "        F.sum(\"order_total\").alias(\"total_dollar_amount\"), \\\n",
        "        F.sum(\"item_count\").alias(\"total_item_count\")) \\\n",
        "        .sort(\"customer_id\", ascending=False)\n",
        "\n",
        "display(df_agg.printSchema())\n",
        "print((df_agg.count(), len(df_agg.columns)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_agg.limit(30))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import pyspark.sql.functions as F \n",
        "\n",
        "# See https://github.com/Azure-Samples/Synapse/blob/main/Notebooks/PySpark/02%20Read%20and%20write%20data%20from%20Azure%20Blob%20Storage%20WASB.ipynb\n",
        "\n",
        "# Azure storage access info\n",
        "blob_account_name   = 'cjoakimstorage'\n",
        "blob_container_name = 'synapse'\n",
        "blob_relative_path  = 'ecomm/'\n",
        "linked_service_name = 'cjoakimstorageAzureBlobStorage'\n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(\n",
        "    linked_service_name)\n",
        "#print('blob_sas_token: {}'.format(blob_sas_token))\n",
        "\n",
        "# Allow Spark to access from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (\n",
        "    blob_container_name, blob_account_name, blob_relative_path)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (\n",
        "    blob_container_name, blob_account_name), blob_sas_token)\n",
        "print('Remote wasbs_path: ' + wasbs_path)\n",
        "\n",
        "csv_path  = '{}{}'.format(wasbs_path,'sales_by_customer_csv')\n",
        "json_path = '{}{}'.format(wasbs_path,'sales_by_customer_json')\n",
        "\n",
        "df_agg.coalesce(1).write.csv(csv_path, mode='overwrite', header='true')\n",
        "#df_agg.coalesce(1).write.json(json_path, mode='overwrite')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write to CosmosDB - linked service 'demoCosmosDB'\n",
        "# See https://docs.microsoft.com/en-us/azure/synapse-analytics/synapse-link/how-to-query-analytical-store-spark#write-spark-dataframe-to-azure-cosmos-db-container\n",
        "\n",
        "df_agg.write.format(\"cosmos.oltp\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"demoCosmosDB\")\\\n",
        "    .option(\"spark.cosmos.container\", \"customer_sales\")\\\n",
        "    .option(\"spak.cosmos.write.upsertenabled\", \"true\")\\\n",
        "    .mode('append')\\\n",
        "    .save()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {}
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}